{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd6df8-8978-40a7-bdfd-25df80f0bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.external import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import timm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader\n",
    "from queue import Queue, Empty\n",
    "import threading\n",
    "from torchvision import transforms\n",
    "import time\n",
    "#import keyboard\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import glob\n",
    "import traceback\n",
    "\n",
    "\n",
    "# FastAI defaults für sicheres Laden\n",
    "import fastai.torch_core\n",
    "fastai.torch_core.defaults.torch_load_kwargs = {'pickle_module': pickle, 'weights_only': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b8775-d450-4abf-86af-a1417d25befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointManager:\n",
    "    def __init__(self, output_manager):\n",
    "        self.output_manager = output_manager\n",
    "        \n",
    "    def wait_for_input(self):\n",
    "        \"\"\"Wartet kurz auf Benutzereingabe\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Epoche beendet. Drücken Sie schnell 'j' + Enter für einen Checkpoint\")\n",
    "        print(f\"oder warten Sie kurz für die nächste Epoche.\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        import sys\n",
    "        import select\n",
    "        \n",
    "        # Kurz auf Eingabe warten\n",
    "        i, o, e = select.select([sys.stdin], [], [], 3)  # 3 Sekunden Timeout\n",
    "        \n",
    "        if i:  # Wenn Eingabe verfügbar\n",
    "            user_input = sys.stdin.readline().strip().lower()\n",
    "            return user_input == 'j'\n",
    "        \n",
    "        print(\"Keine Eingabe - Training wird fortgesetzt...\")\n",
    "        return False\n",
    "            \n",
    "    def save_checkpoint(self, model, optimizer, epoch, metrics_dict):\n",
    "        \"\"\"Speichert den aktuellen Zustand des Trainings\"\"\"\n",
    "        try:\n",
    "            print(\"\\nStarte Checkpoint-Speicherung...\")\n",
    "            \n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            checkpoint_path = self.output_manager.get_path(\n",
    "                'models', \n",
    "                f'checkpoint_{timestamp}.pth'\n",
    "            )\n",
    "            \n",
    "            # Checkpoint-Daten vorbereiten\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'metrics': metrics_dict\n",
    "            }\n",
    "            \n",
    "            # Verzeichnis erstellen\n",
    "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "            \n",
    "            # Checkpoint speichern\n",
    "            print(f\"Speichere nach: {checkpoint_path}\")\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            \n",
    "            if os.path.exists(checkpoint_path):\n",
    "                file_size = os.path.getsize(checkpoint_path)\n",
    "                print(f\"Checkpoint erfolgreich gespeichert! ({file_size/1024/1024:.1f} MB)\")\n",
    "                from fastai.learner import CancelTrainException\n",
    "                raise CancelTrainException()\n",
    "                \n",
    "            print(\"Fehler: Checkpoint-Datei wurde nicht erstellt!\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            if isinstance(e, CancelTrainException):\n",
    "                raise\n",
    "            print(f\"Fehler beim Speichern des Checkpoints: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdabae4-2e89-424a-941c-57af70507791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "        \"\"\"Bereinigt den GPU-Speicher\"\"\"\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc2b04-d205-453e-a0d5-4fc41dbc2322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputManager:\n",
    "    def __init__(self):\n",
    "        # Erstelle Hauptverzeichnis mit Zeitstempel\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.output_dir = f\"run_{timestamp}\"\n",
    "        \n",
    "        # Erstelle Unterverzeichnisse\n",
    "        self.create_subdirs()\n",
    "        \n",
    "        print(f\"Ausgabeverzeichnis erstellt: {self.output_dir}\")\n",
    "    \n",
    "    def create_subdirs(self):\n",
    "        \"\"\"Erstellt die benötigten Unterverzeichnisse\"\"\"\n",
    "        self.dirs = {\n",
    "            'models': os.path.join(self.output_dir, 'models'),\n",
    "            'plots': os.path.join(self.output_dir, 'plots'),\n",
    "            'metrics': os.path.join(self.output_dir, 'metrics'),\n",
    "            'predictions': os.path.join(self.output_dir, 'predictions')\n",
    "        }\n",
    "        \n",
    "        # Erstelle alle Verzeichnisse\n",
    "        for dir_path in self.dirs.values():\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            print(f\"Verzeichnis erstellt: {dir_path}\")\n",
    "    \n",
    "    def get_path(self, category, filename):\n",
    "        \"\"\"Gibt den vollständigen Pfad für eine Datei zurück\"\"\"\n",
    "        if category not in self.dirs:\n",
    "            raise ValueError(f\"Unbekannte Kategorie: {category}\")\n",
    "            \n",
    "        # Stelle sicher, dass das Verzeichnis existiert\n",
    "        os.makedirs(self.dirs[category], exist_ok=True)\n",
    "        \n",
    "        return os.path.join(self.dirs[category], filename)\n",
    "    \n",
    "    def ensure_dir_exists(self, path):\n",
    "        \"\"\"Stellt sicher, dass ein Verzeichnis existiert\"\"\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d25cd4-45fa-4751-900d-0ceda8bd8632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modellklassen auf Modulebene definieren\n",
    "class CustomHead(nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(n_features, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.global_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class MultiLabelModel(nn.Module):\n",
    "    def __init__(self, backbone, head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.head(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06e248-545c-4959-afc4-281f3de8e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRSchedulerCallback(Callback):\n",
    "    def __init__(self, schedule_type, **kwargs):\n",
    "        self.schedule_type = schedule_type\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "    def before_fit(self):\n",
    "        if self.schedule_type == 'cosine':\n",
    "            self.scheduler = partial(\n",
    "                torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "                T_max=self.learn.n_epoch,\n",
    "                eta_min=self.kwargs.get('eta_min', 1e-7)\n",
    "            )\n",
    "        elif self.schedule_type == 'cosine_warmup':\n",
    "            self.scheduler = partial(\n",
    "                torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "                T_0=self.kwargs.get('T_0', 10),\n",
    "                T_mult=self.kwargs.get('T_mult', 2)\n",
    "            )\n",
    "        elif self.schedule_type == 'step':\n",
    "            self.scheduler = partial(\n",
    "                torch.optim.lr_scheduler.StepLR,\n",
    "                step_size=self.kwargs.get('step_size', 30),\n",
    "                gamma=self.kwargs.get('gamma', 0.1)\n",
    "            )\n",
    "        elif self.schedule_type == 'reduce_on_plateau':\n",
    "            self.scheduler = partial(\n",
    "                torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                mode='min',\n",
    "                factor=self.kwargs.get('factor', 0.1),\n",
    "                patience=self.kwargs.get('patience', 10),\n",
    "                verbose=True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unbekannter Schedule-Typ: {self.schedule_type}\")\n",
    "            \n",
    "        self.learn.opt.lr_scheduler = self.scheduler(self.learn.opt.opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2fffbf-10b9-4601-942e-523a67ccccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelMetrics(Callback):\n",
    "    def __init__(self, labels, bbox_df, image_dir, output_manager):\n",
    "        self.labels = labels\n",
    "        self.bbox_df = bbox_df\n",
    "        self.image_dir = image_dir\n",
    "        self.output_manager = output_manager\n",
    "        self.epoch_metrics = {\n",
    "            'mean_ap': [],\n",
    "            'mean_roc_auc': [],\n",
    "            'overall_bias': [],\n",
    "            'per_class_ap': {label: [] for label in labels},\n",
    "            'per_class_roc': {label: [] for label in labels}\n",
    "        }\n",
    "        \n",
    "    def get_bboxes_for_image(self, image_name):\n",
    "        \"\"\"Extrahiert Bounding Boxes für ein bestimmtes Bild\"\"\"\n",
    "        try:\n",
    "            image_bboxes = self.bbox_df[self.bbox_df['Image Index'] == image_name]\n",
    "            boxes = []\n",
    "            for _, row in image_bboxes.iterrows():\n",
    "                boxes.append({\n",
    "                    'x': row['Bbox [x'],\n",
    "                    'y': row['y'],\n",
    "                    'w': row['w'],\n",
    "                    'h': row['h]'],\n",
    "                    'label': row['Finding Label']\n",
    "                })\n",
    "            return boxes\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Laden der Bounding Boxes für {image_name}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Erstellt und speichert Plots für Loss und Metriken\"\"\"\n",
    "        try:\n",
    "            if not hasattr(self, 'mean_ap_history') or len(self.mean_ap_history) == 0:\n",
    "                print(\"Keine Metrik-Daten für Plots verfügbar\")\n",
    "                return\n",
    "                \n",
    "            # Korrekte Epochen-Liste erstellen, die zur Länge der gespeicherten Daten passt\n",
    "            num_epochs = len(self.mean_ap_history)\n",
    "            epochs = list(range(1, num_epochs + 1))\n",
    "            \n",
    "            # Prüfe, ob alle Listen gleich lang sind\n",
    "            if hasattr(self, 'train_losses') and len(self.train_losses) > 0:\n",
    "                if len(self.train_losses) != num_epochs:\n",
    "                    print(f\"Warnung: train_losses hat Länge {len(self.train_losses)}, mean_ap_history hat Länge {num_epochs}\")\n",
    "                    # Synchronisiere die Längen\n",
    "                    if len(self.train_losses) > num_epochs:\n",
    "                        self.train_losses = self.train_losses[:num_epochs]\n",
    "                        self.valid_losses = self.valid_losses[:num_epochs]\n",
    "                    \n",
    "                # 1. Plot für Train und Validation Loss\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.plot(epochs, self.train_losses, 'b-', label='Training Loss')\n",
    "                plt.plot(epochs, self.valid_losses, 'r-', label='Validation Loss')\n",
    "                plt.title('Training und Validation Loss')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                \n",
    "                loss_plot_path = self.output_manager.get_path('plots', f'loss_epoch_{num_epochs}.png')\n",
    "                plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            # 2. Plot für Mean AP und ROC AUC\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(epochs, self.mean_ap_history, 'g-', label='Mean AP')\n",
    "            plt.plot(epochs, self.mean_roc_history, 'm-', label='Mean ROC AUC')\n",
    "            plt.title('Mean AP und ROC AUC')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Score')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            metrics_plot_path = self.output_manager.get_path('plots', f'metrics_epoch_{num_epochs}.png')\n",
    "            plt.savefig(metrics_plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # 3. Plot für Overall Bias\n",
    "            if hasattr(self, 'overall_bias_history') and len(self.overall_bias_history) > 0:\n",
    "                if len(self.overall_bias_history) != num_epochs:\n",
    "                    print(f\"Warnung: overall_bias_history hat Länge {len(self.overall_bias_history)}, mean_ap_history hat Länge {num_epochs}\")\n",
    "                    # Verwende nur so viele Datenpunkte wie in epochs\n",
    "                    bias_data = self.overall_bias_history[:num_epochs]\n",
    "                else:\n",
    "                    bias_data = self.overall_bias_history\n",
    "                    \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.plot(epochs, bias_data, 'r-')\n",
    "                plt.title('Overall Bias über Epochen')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Bias (%)')\n",
    "                plt.grid(True)\n",
    "                \n",
    "                bias_plot_path = self.output_manager.get_path('plots', f'bias_epoch_{num_epochs}.png')\n",
    "                plt.savefig(bias_plot_path, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            print(f\"Plots gespeichert in {self.output_manager.dirs['plots']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Erstellen der Plots: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    def plot_performance_curves(self):\n",
    "        \"\"\"Erstellt Performance-Plots\"\"\"\n",
    "        try:\n",
    "            # Sicherheitscheck hinzufügen\n",
    "            if len(self.epoch_metrics['mean_ap']) == 0:\n",
    "                print(\"Keine Metrikdaten für Performance-Plots verfügbar\")\n",
    "                return\n",
    "            epochs = range(1, len(self.epoch_metrics['mean_ap']) + 1)\n",
    "            \n",
    "            # Erstelle Figure mit 3 Subplots\n",
    "            plt.figure(figsize=(15, 12))\n",
    "            \n",
    "            # Plot 1: Durchschnittliche Metriken\n",
    "            plt.subplot(3, 1, 1)\n",
    "            plt.plot(epochs, self.epoch_metrics['mean_ap'], 'b-', label='Mean AP')\n",
    "            plt.plot(epochs, self.epoch_metrics['mean_roc_auc'], 'r-', label='Mean ROC AUC')\n",
    "            plt.title('Durchschnittliche Performance-Metriken über Epochen')\n",
    "            plt.xlabel('Epoche')\n",
    "            plt.ylabel('Score')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Plot 2: Per-Class AP Scores\n",
    "            plt.subplot(3, 1, 2)\n",
    "            for label in self.labels:\n",
    "                plt.plot(epochs, self.epoch_metrics['per_class_ap'][label], \n",
    "                        label=f'{label[:10]}...' if len(label) > 10 else label)\n",
    "            plt.title('Average Precision pro Klasse')\n",
    "            plt.xlabel('Epoche')\n",
    "            plt.ylabel('AP Score')\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Plot 3: Overall Bias\n",
    "            plt.subplot(3, 1, 3)\n",
    "            plt.plot(epochs, self.epoch_metrics['overall_bias'], 'g-')\n",
    "            plt.title('Gesamtabweichung (Bias) über Epochen')\n",
    "            plt.xlabel('Epoche')\n",
    "            plt.ylabel('Absolute Abweichung (%)')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\n",
    "                self.output_manager.get_path('plots', f'performance_epoch_{len(epochs)}.png'),\n",
    "                bbox_inches='tight', \n",
    "                dpi=300\n",
    "            )\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Erstellen der Performance-Plots: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def visualize_predictions(self, n_samples=6):\n",
    "        \"\"\"Visualisiert Vorhersagen für zufällige Bilder\"\"\"\n",
    "        try:\n",
    "            if not hasattr(self.learn.dls, 'train') or not hasattr(self.learn.dls, 'valid'):\n",
    "                print(\"Keine Datenlader verfügbar\")\n",
    "                return\n",
    "                \n",
    "            # Erstelle eine einzelne Figur für alle Plots\n",
    "            fig, axes = plt.subplots(2, n_samples, figsize=(20, 10))\n",
    "            fig.suptitle('Trainings- und Validierungsvorhersagen', fontsize=16)\n",
    "            \n",
    "            def process_batch(dl, row_idx, axes_row):\n",
    "                try:\n",
    "                    # Hole einen Batch und stelle sicher, dass er gültig ist\n",
    "                    batch = dl.one_batch()\n",
    "                    if not isinstance(batch, (tuple, list)) or len(batch) < 2:\n",
    "                        print(f\"Ungültiger Batch-Format: {type(batch)}\")\n",
    "                        return\n",
    "                        \n",
    "                    images, labels = batch\n",
    "                    if len(images) == 0:\n",
    "                        print(\"Keine Bilder im Batch\")\n",
    "                        return\n",
    "                    \n",
    "                    # Vorhersagen für den Batch\n",
    "                    self.learn.model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        predictions = torch.sigmoid(self.learn.model(images))\n",
    "                    \n",
    "                    # Zufällige Indizes für die Visualisierung\n",
    "                    # Beschränke n_samples auf verfügbare Bilder\n",
    "                    actual_n_samples = min(n_samples, len(images))\n",
    "                    indices = torch.randperm(len(images))[:actual_n_samples]\n",
    "                    \n",
    "                    for col_idx, idx in enumerate(indices):\n",
    "                        try:\n",
    "                            ax = axes_row[col_idx]\n",
    "                        \n",
    "                            # Bild denormalisieren und anzeigen\n",
    "                            img = images[idx].cpu()\n",
    "                            \n",
    "                            # Denormalisierung für ConvNeXt\n",
    "                            mean = torch.tensor([0.5, 0.5, 0.5]).view(-1, 1, 1)\n",
    "                            std = torch.tensor([0.5, 0.5, 0.5]).view(-1, 1, 1)\n",
    "                            img = img * std + mean\n",
    "                            \n",
    "                            # Konvertiere zu Bild-Format und klemme Werte\n",
    "                            img = img.permute(1, 2, 0)  # CHW -> HWC\n",
    "                            img = torch.clamp(img, 0, 1)  # Beschränke auf [0,1]\n",
    "                            \n",
    "                            ax.imshow(img)\n",
    "                            \n",
    "                            # Versuche den Bildnamen zu extrahieren\n",
    "                            try:\n",
    "                                if hasattr(dl.dataset, 'items') and hasattr(dl, 'indices'):\n",
    "                                    data_idx = dl.indices[idx] if idx < len(dl.indices) else None\n",
    "                                    if data_idx is not None and data_idx < len(dl.dataset.items):\n",
    "                                        image_name = dl.dataset.items[data_idx].get('Image Index', 'unknown')\n",
    "                                    else:\n",
    "                                        image_name = 'unknown'\n",
    "                                else:\n",
    "                                    image_name = 'unknown'\n",
    "                            \n",
    "                                # Bounding Boxes zeichnen\n",
    "                                if image_name != 'unknown':\n",
    "                                    bboxes = self.get_bboxes_for_image(image_name)\n",
    "                                    for bbox in bboxes:\n",
    "                                        rect = plt.Rectangle(\n",
    "                                            (bbox['x'], bbox['y']),\n",
    "                                            bbox['w'],\n",
    "                                            bbox['h'],\n",
    "                                            fill=False,\n",
    "                                            edgecolor='red',\n",
    "                                            linewidth=2\n",
    "                                        )\n",
    "                                        ax.add_patch(rect)\n",
    "                                        ax.text(\n",
    "                                            bbox['x'],\n",
    "                                            bbox['y'] - 5,\n",
    "                                            bbox['label'],\n",
    "                                            color='red',\n",
    "                                            fontsize=8,\n",
    "                                            bbox=dict(facecolor='white', alpha=0.7)\n",
    "                                        )\n",
    "                            except Exception as e:\n",
    "                                print(f\"Fehler beim Zeichnen der Bounding Boxes: {e}\")\n",
    "                            \n",
    "                            # Labels verarbeiten\n",
    "                            try:\n",
    "                                true_labels = [self.labels[j] for j in range(len(self.labels)) \n",
    "                                             if idx < len(labels) and j < len(labels[idx]) and labels[idx][j] == 1]\n",
    "                                pred_labels = [self.labels[j] for j in range(len(self.labels)) \n",
    "                                             if j < len(predictions[idx]) and predictions[idx][j] > 0.5]\n",
    "                                \n",
    "                                # Labels als Text hinzufügen\n",
    "                                ax.set_title(\n",
    "                                    f'Wahr: {\", \".join(true_labels[:3])}\\n' + \n",
    "                                    f'Vorher.: {\", \".join(pred_labels[:3])}',\n",
    "                                    fontsize=8\n",
    "                                )\n",
    "                            except Exception as e:\n",
    "                                print(f\"Fehler bei der Label-Verarbeitung: {e}\")\n",
    "                                ax.set_title(\"Fehler bei Labels\", fontsize=8)\n",
    "                                \n",
    "                            ax.axis('off')\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Fehler bei der Verarbeitung von Bild {idx}: {e}\")\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Fehler bei der Batch-Verarbeitung: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "            \n",
    "            # Trainingsbilder in der ersten Reihe\n",
    "            process_batch(self.learn.dls.train, 0, axes[0])\n",
    "            axes[0, 0].set_ylabel('Training', fontsize=12)\n",
    "            \n",
    "            # Validierungsbilder in der zweiten Reihe\n",
    "            process_batch(self.learn.dls.valid, 1, axes[1])\n",
    "            axes[1, 0].set_ylabel('Validierung', fontsize=12)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\n",
    "                self.output_manager.get_path('predictions', f'predictions_epoch_{len(self.epoch_metrics[\"mean_ap\"])}.png'),\n",
    "                bbox_inches='tight',\n",
    "                dpi=300\n",
    "            )\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei der Visualisierung der Vorhersagen: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def update_metrics_csv(self):\n",
    "        \"\"\"Aktualisiert CSV-Datei mit Metriken für alle Epochen\"\"\"\n",
    "\n",
    "        # Prüfe, ob Metrik-Daten verfügbar sind\n",
    "        if len(self.epoch_metrics['mean_ap']) == 0:\n",
    "            print(\"Keine Metrikdaten für CSV-Update verfügbar\")\n",
    "            return\n",
    "            \n",
    "        csv_path = self.output_manager.get_path('metrics', 'training_metrics.csv')\n",
    "\n",
    "        # Prüfe, ob Metrik-Daten verfügbar sind\n",
    "        if len(self.epoch_metrics['mean_ap']) == 0:\n",
    "            print(\"Keine Metrikdaten für CSV-Update verfügbar\")\n",
    "            return\n",
    "        \n",
    "        # Erstelle Header falls Datei nicht existiert\n",
    "        if not os.path.exists(csv_path):\n",
    "            header = ['epoch', 'mean_ap', 'mean_roc_auc', 'overall_bias', 'train_loss', 'valid_loss']\n",
    "            header.extend([f'{label}_actual' for label in self.labels])\n",
    "            header.extend([f'{label}_predicted' for label in self.labels])\n",
    "            \n",
    "            with open(csv_path, 'w') as f:\n",
    "                f.write(','.join(header) + '\\n')\n",
    "        \n",
    "        # Aktuelle Werte hinzufügen\n",
    "        epoch = len(self.epoch_metrics['mean_ap'])\n",
    "        mean_ap = self.epoch_metrics['mean_ap'][-1]\n",
    "        mean_roc = self.epoch_metrics['mean_roc_auc'][-1]\n",
    "        overall_bias = self.epoch_metrics['overall_bias'][-1]\n",
    "        \n",
    "        train_loss = float(self.learn.recorder.values[-1][0]) if self.learn.recorder.values else None\n",
    "        valid_loss = float(self.learn.recorder.values[-1][1]) if self.learn.recorder.values else None\n",
    "        \n",
    "        # Klassen-Verteilungen\n",
    "        actual_values = []\n",
    "        predicted_values = []\n",
    "        \n",
    "        # Prüfe, ob die Verteilungen verfügbar sind\n",
    "        if hasattr(self, 'last_target_distribution') and hasattr(self, 'last_pred_distribution'):\n",
    "            for i, label in enumerate(self.labels):\n",
    "                true_dist = self.last_target_distribution[i].item() * 100 if i < len(self.last_target_distribution) else 0\n",
    "                pred_dist = self.last_pred_distribution[i].item() * 100 if i < len(self.last_pred_distribution) else 0\n",
    "                actual_values.append(str(true_dist))\n",
    "                predicted_values.append(str(pred_dist))\n",
    "        else:\n",
    "            # Wenn keine Verteilungen verfügbar sind, fülle mit 0\n",
    "            for _ in self.labels:\n",
    "                actual_values.append(\"0\")\n",
    "                predicted_values.append(\"0\")\n",
    "        \n",
    "        # Zeile zusammenbauen\n",
    "        row = [str(epoch), str(mean_ap), str(mean_roc), str(overall_bias), str(train_loss), str(valid_loss)]\n",
    "        row.extend(actual_values)\n",
    "        row.extend(predicted_values)\n",
    "        \n",
    "        # An CSV anhängen\n",
    "        with open(csv_path, 'a') as f:\n",
    "            f.write(','.join(row) + '\\n')\n",
    "\n",
    "    def plot_and_save_all_metrics(self):\n",
    "        \"\"\"Erstellt und speichert umfassende Plots für alle Metriken\"\"\"\n",
    "        \n",
    "         # Sicherheitscheck \n",
    "        if len(self.epoch_metrics['mean_ap']) == 0:\n",
    "            print(\"Keine Metrikdaten für umfassende Plots verfügbar\")\n",
    "            return\n",
    "            \n",
    "        # Performance-Metriken\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        plt.subplot(2, 2, 1)\n",
    "        epochs = range(1, len(self.epoch_metrics['mean_ap'])+1)\n",
    "        plt.plot(epochs, self.epoch_metrics['mean_ap'], 'b-', label='Mean AP')\n",
    "        plt.plot(epochs, self.epoch_metrics['mean_roc_auc'], 'g-', label='Mean ROC AUC')\n",
    "        plt.title('Performance-Metriken')\n",
    "        plt.xlabel('Epoche')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Loss-Kurven\n",
    "        plt.subplot(2, 2, 2)\n",
    "        train_losses = [x[0] for x in self.learn.recorder.values]\n",
    "        valid_losses = [x[1] for x in self.learn.recorder.values]\n",
    "        plt.plot(range(1, len(train_losses)+1), train_losses, 'b-', label='Train Loss')\n",
    "        plt.plot(range(1, len(valid_losses)+1), valid_losses, 'r-', label='Valid Loss')\n",
    "        plt.title('Loss-Kurven')\n",
    "        plt.xlabel('Epoche')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Klassenverteilung der letzten Epoche\n",
    "        plt.subplot(2, 2, 3)\n",
    "        \n",
    "        # Verwende einen sicheren Ansatz - prüfe, ob die Verteilungsvariablen existieren\n",
    "        if hasattr(self, 'last_target_distribution') and hasattr(self, 'last_pred_distribution'):\n",
    "            top_classes = sorted(range(len(self.labels)), \n",
    "                             key=lambda i: self.last_target_distribution[i].item() if i < len(self.last_target_distribution) else 0,\n",
    "                             reverse=True)[:8]  # Top 8 Klassen\n",
    "            \n",
    "            x = range(len(top_classes))\n",
    "            width = 0.35\n",
    "            true_vals = [self.last_target_distribution[i].item() * 100 if i < len(self.last_target_distribution) else 0 for i in top_classes]\n",
    "            pred_vals = [self.last_pred_distribution[i].item() * 100 if i < len(self.last_pred_distribution) else 0 for i in top_classes]\n",
    "            \n",
    "            plt.bar([p - width/2 for p in x], true_vals, width, label='Tatsächlich')\n",
    "            plt.bar([p + width/2 for p in x], pred_vals, width, label='Vorhergesagt')\n",
    "            plt.xticks(x, [self.labels[i] for i in top_classes], rotation=45, ha='right')\n",
    "            plt.title('Klassenverteilung (Epoche {})'.format(len(self.epoch_metrics['mean_ap'])))\n",
    "            plt.ylabel('Prozent')\n",
    "            plt.legend()\n",
    "        else:\n",
    "            # Wenn keine Daten verfügbar sind, zeige einen Platzhaltertext\n",
    "            plt.text(0.5, 0.5, \"Keine Verteilungsdaten verfügbar\", \n",
    "                    horizontalalignment='center', verticalalignment='center')\n",
    "            plt.title('Klassenverteilung (keine Daten)')\n",
    "        \n",
    "        # Overall Bias über Epochen\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(epochs, self.epoch_metrics['overall_bias'], 'r-')\n",
    "        plt.title('Overall Bias')\n",
    "        plt.xlabel('Epoche')\n",
    "        plt.ylabel('Prozent')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_manager.get_path('plots', f'all_metrics_epoch_{len(self.epoch_metrics[\"mean_ap\"])}.png'),\n",
    "                   bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not hasattr(self, 'predictions'):\n",
    "            self.predictions = []\n",
    "            self.targets = []\n",
    "        \n",
    "        if self.learn.training:\n",
    "            return\n",
    "        \n",
    "        preds = self.learn.pred\n",
    "        targets = self.learn.y\n",
    "        self.predictions.append(preds.detach().cpu())\n",
    "        self.targets.append(targets.detach().cpu())\n",
    "        \n",
    "    def after_epoch(self):\n",
    "        if len(self.predictions) == 0:\n",
    "            return\n",
    "                \n",
    "        try:\n",
    "            preds = torch.sigmoid(torch.cat(self.predictions))\n",
    "            targets = torch.cat(self.targets)\n",
    "    \n",
    "            # Berechne Metriken...\n",
    "            metrics_dict = {\n",
    "                'average_precision': {},\n",
    "                'roc_auc': {},\n",
    "                'class_distribution': {},\n",
    "                'prediction_distribution': {}\n",
    "            }\n",
    "            \n",
    "            # Berechne Vorhersageverteilung\n",
    "            pred_binary = (preds > 0.5).float()\n",
    "            pred_distribution = pred_binary.mean(dim=0)\n",
    "            target_distribution = targets.float().mean(dim=0)\n",
    "\n",
    "            # Speichere die Verteilungen für andere Methoden\n",
    "            self.last_target_distribution = target_distribution\n",
    "            self.last_pred_distribution = pred_distribution\n",
    "            \n",
    "            print(\"\\nVerteilungsanalyse der Krankheiten:\")\n",
    "            print(\"--------------------------------\")\n",
    "            \n",
    "            # Sicherstellen, dass die Indizes korrekt sind\n",
    "            for i, label in enumerate(self.labels):\n",
    "                if i >= len(pred_distribution) or i >= len(target_distribution):\n",
    "                    print(f\"Warnung: Index {i} außerhalb des gültigen Bereichs\")\n",
    "                    continue\n",
    "                    \n",
    "                true_dist = target_distribution[i].item() * 100\n",
    "                pred_dist = pred_distribution[i].item() * 100\n",
    "                bias = pred_dist - true_dist\n",
    "                \n",
    "                print(f\"\\n{label}:\")\n",
    "                print(f\"  Anteil in Daten: {true_dist:.1f}%\")\n",
    "                print(f\"  Anteil in Vorhersagen: {pred_dist:.1f}%\")\n",
    "                print(f\"  Abweichung: {bias:+.1f}%\")\n",
    "                \n",
    "                metrics_dict['prediction_distribution'][label] = {\n",
    "                    'true_distribution': true_dist,\n",
    "                    'predicted_distribution': pred_dist,\n",
    "                    'bias': bias\n",
    "                }\n",
    "                \n",
    "                # Performance Metriken\n",
    "                try:\n",
    "                    if i < targets.shape[1] and i < preds.shape[1]:\n",
    "                        # Prüfe, ob positive Beispiele vorhanden sind\n",
    "                        if torch.sum(targets[:, i]) > 0:\n",
    "                            ap = average_precision_score(targets[:, i], preds[:, i])\n",
    "                            metrics_dict['average_precision'][label] = float(ap)\n",
    "                            self.epoch_metrics['per_class_ap'][label].append(float(ap))\n",
    "                        else:\n",
    "                            print(f\"Hinweis: Keine positiven Beispiele für {label} in diesem Batch\")\n",
    "                            metrics_dict['average_precision'][label] = None\n",
    "                            self.epoch_metrics['per_class_ap'][label].append(0.0)\n",
    "                except Exception as e:\n",
    "                    print(f\"Fehler bei AP Score für {label}: {str(e)}\")\n",
    "                    metrics_dict['average_precision'][label] = None\n",
    "                    self.epoch_metrics['per_class_ap'][label].append(0.0)\n",
    "                \n",
    "                try:\n",
    "                    if i < targets.shape[1] and i < preds.shape[1]:\n",
    "                        # Prüfe, ob positive und negative Beispiele vorhanden sind\n",
    "                        unique_values = torch.unique(targets[:, i])\n",
    "                        if len(unique_values) > 1:\n",
    "                            roc = roc_auc_score(targets[:, i], preds[:, i])\n",
    "                            metrics_dict['roc_auc'][label] = float(roc)\n",
    "                            self.epoch_metrics['per_class_roc'][label].append(float(roc))\n",
    "                        else:\n",
    "                            value_type = \"keine\" if len(unique_values) == 0 else (\"nur positive\" if unique_values[0] == 1 else \"nur negative\")\n",
    "                            print(f\"Hinweis: {value_type} Beispiele für {label} in diesem Batch\")\n",
    "                            metrics_dict['roc_auc'][label] = None\n",
    "                            self.epoch_metrics['per_class_roc'][label].append(0.0)\n",
    "                except Exception as e:\n",
    "                    print(f\"Fehler bei ROC AUC für {label}: {str(e)}\")\n",
    "                    metrics_dict['roc_auc'][label] = None\n",
    "                    self.epoch_metrics['per_class_roc'][label].append(0.0)\n",
    "            \n",
    "            # Durchschnittsmetriken\n",
    "            valid_ap_scores = [score for score in metrics_dict['average_precision'].values() if score is not None]\n",
    "            valid_roc_scores = [score for score in metrics_dict['roc_auc'].values() if score is not None]\n",
    "            \n",
    "            mean_ap = np.mean(valid_ap_scores) if valid_ap_scores else 0.0\n",
    "            mean_roc = np.mean(valid_roc_scores) if valid_roc_scores else 0.0\n",
    "            overall_bias = float(torch.abs(pred_distribution - target_distribution).mean() * 100)\n",
    "\n",
    "            # Loss-Werte für Plots speichern\n",
    "            if hasattr(self.learn.recorder, 'values') and len(self.learn.recorder.values) > 0:\n",
    "                if not hasattr(self, 'train_losses'):\n",
    "                    self.train_losses = []\n",
    "                    self.valid_losses = []\n",
    "                    \n",
    "                # Extrahiere die neuesten Loss-Werte\n",
    "                latest_record = self.learn.recorder.values[-1]\n",
    "                if len(latest_record) >= 2:\n",
    "                    self.train_losses.append(float(latest_record[0]))\n",
    "                    self.valid_losses.append(float(latest_record[1]))\n",
    "            \n",
    "            # Metriken für Plots speichern\n",
    "            if not hasattr(self, 'mean_ap_history'):\n",
    "                self.mean_ap_history = []\n",
    "                self.mean_roc_history = []\n",
    "                self.overall_bias_history = []\n",
    "                \n",
    "            self.mean_ap_history.append(mean_ap)\n",
    "            self.mean_roc_history.append(mean_roc)\n",
    "            self.overall_bias_history.append(overall_bias)\n",
    "\n",
    "            # Nach Berechnung der Metriken\n",
    "            epoch_metrics = {\n",
    "                'epoch': len(self.epoch_metrics['mean_ap']),\n",
    "                'mean_ap': float(mean_ap),\n",
    "                'mean_roc_auc': float(mean_roc),\n",
    "                'overall_bias': float(overall_bias),\n",
    "                'class_distribution': {\n",
    "                    label: {\n",
    "                        'actual': float(true_dist),\n",
    "                        'predicted': float(pred_dist),\n",
    "                        'bias': float(bias)\n",
    "                    } for i, label in enumerate(self.labels)\n",
    "                    if i < len(pred_distribution) and i < len(target_distribution)\n",
    "                },\n",
    "                'loss': {\n",
    "                    'train': float(self.learn.recorder.values[-1][0]) if self.learn.recorder.values else None,\n",
    "                    'valid': float(self.learn.recorder.values[-1][1]) if self.learn.recorder.values else None\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Metriken in csv speichern\n",
    "            self.update_metrics_csv()\n",
    "            \n",
    "            # Speichere Metriken als JSON\n",
    "            metrics_path = self.output_manager.get_path('metrics', f'epoch_{len(self.epoch_metrics[\"mean_ap\"])}_metrics.json')\n",
    "            with open(metrics_path, 'w') as f:\n",
    "                json.dump(epoch_metrics, f, indent=2)\n",
    "            \n",
    "            # Recorder-Werte verarbeiten\n",
    "            values = []\n",
    "            for x in self.learn.recorder.values:\n",
    "                if x is not None:\n",
    "                    try:\n",
    "                        # X ist eine Liste von [train_loss, valid_loss, accuracy]\n",
    "                        values.append({\n",
    "                            'train_loss': float(x[0]),\n",
    "                            'valid_loss': float(x[1]),\n",
    "                            'accuracy': float(x[2])\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Fehler beim Konvertieren der Values: {str(e)}\")\n",
    "            \n",
    "            \n",
    "            metrics_dict['mean_ap'] = mean_ap\n",
    "            metrics_dict['mean_roc_auc'] = mean_roc\n",
    "            metrics_dict['overall_bias'] = overall_bias\n",
    "            metrics_dict['validation_values'] = values\n",
    "            \n",
    "            self.epoch_metrics['mean_ap'].append(mean_ap)\n",
    "            self.epoch_metrics['mean_roc_auc'].append(mean_roc)\n",
    "            self.epoch_metrics['overall_bias'].append(overall_bias)\n",
    "            \n",
    "            print(f\"\\nEpoche {len(self.epoch_metrics['mean_ap'])}:\")\n",
    "            print(f\"Mean AP: {mean_ap:.3f}\")\n",
    "            print(f\"Mean ROC AUC: {mean_roc:.3f}\")\n",
    "            print(f\"Overall Bias: {overall_bias:.1f}%\")\n",
    "            \n",
    "            # Metrics im Learner speichern\n",
    "            self.learn.metrics_dict = metrics_dict\n",
    "            \n",
    "            # Plots aktualisieren\n",
    "            #self.plot_metrics()\n",
    "            self.plot_performance_curves()\n",
    "            self.visualize_predictions()\n",
    "            self.plot_and_save_all_metrics()\n",
    "            \n",
    "            # Checkpoint-Info ausgeben\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"Epoche {len(self.epoch_metrics['mean_ap'])} abgeschlossen\")\n",
    "            print(f\"Mean AP: {mean_ap:.3f}\")\n",
    "            print(f\"Mean ROC AUC: {mean_roc:.3f}\")\n",
    "            print(f\"Overall Bias: {overall_bias:.1f}%\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # Automatisches Speichern des Checkpoints\n",
    "            checkpoint_mgr = CheckpointManager(self.output_manager)\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            checkpoint_path = self.output_manager.get_path(\n",
    "                'models', \n",
    "                f'checkpoint_epoch_{len(self.epoch_metrics[\"mean_ap\"])}_{timestamp}.pth'\n",
    "            )\n",
    "            \n",
    "            # Checkpoint-Daten vorbereiten\n",
    "            checkpoint = {\n",
    "                'epoch': len(self.epoch_metrics['mean_ap']),\n",
    "                'model_state_dict': self.learn.model.state_dict(),\n",
    "                'optimizer_state_dict': self.learn.opt.state_dict(),\n",
    "                'metrics': self.learn.metrics_dict\n",
    "            }\n",
    "            \n",
    "            # Verzeichnis erstellen\n",
    "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "            \n",
    "            # Checkpoint speichern\n",
    "            print(f\"Speichere Modell nach: {checkpoint_path}\")\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            \n",
    "            if os.path.exists(checkpoint_path):\n",
    "                file_size = os.path.getsize(checkpoint_path)\n",
    "                print(f\"Checkpoint erfolgreich gespeichert! ({file_size/1024/1024:.1f} MB)\")\n",
    "                print(\"Training wird fortgesetzt...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler in after_epoch: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            # Listen zurücksetzen\n",
    "            self.predictions = []\n",
    "            self.targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b59ae-88fe-4370-9cc3-fbab9ab80473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungXRayDataset:\n",
    "    def __init__(self, image_dir, labels_csv, bbox_csv, aug_dir=None, aug_csv=None, max_samples=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.aug_dir = aug_dir  # Verzeichnis für augmentierte Bilder\n",
    "        \n",
    "        # Lade Labels\n",
    "        self.labels_df = pd.read_csv(labels_csv)\n",
    "        \n",
    "        # Lade augmentierte Labels, falls vorhanden\n",
    "        if aug_csv is not None and os.path.exists(aug_csv):\n",
    "            aug_df = pd.read_csv(aug_csv)\n",
    "            self.labels_df = pd.concat([self.labels_df, aug_df], ignore_index=True)\n",
    "            print(f\"Kombinierten Datensatz erstellt mit {len(self.labels_df)} Bildern\")\n",
    "        \n",
    "        if max_samples is not None:\n",
    "            self.labels_df = self.labels_df.sample(\n",
    "                n=min(max_samples, len(self.labels_df)), \n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "        # HIER DAS BILDPFAD-CACHING EINFÜGEN:\n",
    "        # Erstelle ein Pfad-Cache für schnellen Zugriff\n",
    "        print(\"Erstelle Cache für Bildpfade...\")\n",
    "        self.image_path_cache = {}\n",
    "        for img_name in self.labels_df['Image Index'].unique():\n",
    "            orig_path = os.path.join(self.image_dir, img_name)\n",
    "            aug_path = os.path.join(self.aug_dir, img_name) if self.aug_dir else None\n",
    "            \n",
    "            if os.path.exists(orig_path):\n",
    "                self.image_path_cache[img_name] = orig_path\n",
    "            elif aug_path and os.path.exists(aug_path):\n",
    "                self.image_path_cache[img_name] = aug_path\n",
    "        print(f\"Pfad-Cache erstellt für {len(self.image_path_cache)} Bilder\")\n",
    "        \n",
    "        self.bbox_df = pd.read_csv(bbox_csv)\n",
    "        \n",
    "        # Extrahiere unique Labels\n",
    "        self.disease_labels = sorted(list(set(\n",
    "            label.strip() \n",
    "            for labels in self.labels_df['Finding Labels'].str.split('|')\n",
    "            for label in labels\n",
    "        )))\n",
    "        \n",
    "        # Optional: Zähle Klassen für informative Zwecke, ohne komplexe Gewichtungsberechnungen\n",
    "        self.class_counts = {}\n",
    "        for labels in self.labels_df['Finding Labels'].str.split('|'):\n",
    "            for label in labels:\n",
    "                label = label.strip()\n",
    "                if label not in self.class_counts:\n",
    "                    self.class_counts[label] = 0\n",
    "                self.class_counts[label] += 1\n",
    "                \n",
    "        # Ausgabe der Statistiken\n",
    "        print(\"Gefundene Krankheiten:\")\n",
    "        for label, count in self.class_counts.items():\n",
    "            print(f\"{label}: {count} Bilder\")\n",
    "    \n",
    "    # Neue Methode zum Suchen von Bildern in verschiedenen Verzeichnissen\n",
    "    def _find_image_path(self, image_name):\n",
    "        \"\"\"Sucht ein Bild im Pfad-Cache\"\"\"\n",
    "        # Zuerst im Cache nachschlagen\n",
    "        if image_name in self.image_path_cache:\n",
    "            return self.image_path_cache[image_name]\n",
    "        \n",
    "        # Falls nicht im Cache (sollte selten vorkommen)\n",
    "        orig_path = os.path.join(self.image_dir, image_name)\n",
    "        if os.path.exists(orig_path):\n",
    "            # Zum Cache hinzufügen und zurückgeben\n",
    "            self.image_path_cache[image_name] = orig_path\n",
    "            return orig_path\n",
    "        \n",
    "        # Falls nicht gefunden und Augmentation-Verzeichnis vorhanden, dort suchen\n",
    "        if self.aug_dir:\n",
    "            aug_path = os.path.join(self.aug_dir, image_name)\n",
    "            if os.path.exists(aug_path):\n",
    "                # Zum Cache hinzufügen und zurückgeben\n",
    "                self.image_path_cache[image_name] = aug_path\n",
    "                return aug_path\n",
    "        \n",
    "        # Wenn nicht gefunden, Original-Pfad zurückgeben (wird einen Fehler auslösen)\n",
    "        return os.path.join(self.image_dir, image_name)\n",
    "\n",
    "    def _calculate_sample_weights(self):\n",
    "        \"\"\"Berechnet Gewichte für jeden Datenpunkt basierend auf seinen Labels\"\"\"\n",
    "        total_samples = len(self.labels_df)\n",
    "        weights = []\n",
    "        \n",
    "        for labels in self.labels_df['Finding Labels'].str.split('|'):\n",
    "            # Gewichte für jeden Datenpunkt basierend auf seinen Labels\n",
    "            label_weights = [total_samples / (self.class_counts[label.strip()] + 1e-5) \n",
    "                           for label in labels]\n",
    "            weights.append(np.mean(label_weights))\n",
    "        \n",
    "        return torch.FloatTensor(weights)\n",
    "    \n",
    "    def get_data(self, image_size=224):\n",
    "        \"\"\"Erstellt einen FastAI DataBlock\"\"\"\n",
    "        return DataBlock(\n",
    "            blocks=(ImageBlock, MultiCategoryBlock(vocab=self.disease_labels)),\n",
    "            get_x=lambda x: self._find_image_path(x['Image Index']),  # Angepasste get_x Funktion\n",
    "            get_y=lambda x: [label.strip() for label in x['Finding Labels'].split('|')],\n",
    "            splitter=RandomSplitter(),\n",
    "            item_tfms=Resize(460),\n",
    "            batch_tfms=[\n",
    "                # Keine Augmentation hier, nur Normalisierung\n",
    "                Normalize.from_stats([0.5]*3, [0.5]*3)  # ConvNeXt Normalisierung\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def create_dataloader(self, batch_size=32, image_size=224, num_workers=6):\n",
    "        \"\"\"Erstellt DataLoader ohne Augmentation-Pipeline\"\"\"\n",
    "        dblock = self.get_data(image_size)\n",
    "        \n",
    "        dls = dblock.dataloaders(\n",
    "            self.labels_df,\n",
    "            bs=batch_size\n",
    "        )\n",
    "        \n",
    "        # Direkt num_workers setzen\n",
    "        dls.train.num_workers = num_workers\n",
    "        dls.valid.num_workers = num_workers\n",
    "\n",
    "        # Debug: Nach DataLoader Erstellung\n",
    "        print(f\"DataLoader settings:\")\n",
    "        print(f\"Training DataLoader workers: {dls.train.num_workers}\")\n",
    "        print(f\"Training DataLoader batch size: {dls.train.bs}\")\n",
    "        \n",
    "        # Berechne Gewichte für Weighted Sampling (behalten wir bei)\n",
    "        weights = self._calculate_sample_weights()\n",
    "        \n",
    "        # Erstelle Weighted Sampler\n",
    "        sampler = torch.utils.data.WeightedRandomSampler(\n",
    "            weights=weights,\n",
    "            num_samples=len(weights),\n",
    "            replacement=True\n",
    "        )\n",
    "        \n",
    "        # Ersetze den Training-DataLoader mit normalem DataLoader und gewichtetem Sampling\n",
    "        # KEIN AugmentedDataLoader mehr!\n",
    "        dls.train.dl = DataLoader(\n",
    "            dls.train.dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0f850-f1b1-46a4-a074-260260c138c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientAccumulation(Callback):\n",
    "    def __init__(self, n_acc=2):\n",
    "        self.n_acc = n_acc\n",
    "    \n",
    "    def before_batch(self):\n",
    "        if not self.training: return\n",
    "        # Teile Loss durch Akkumulationsschritte\n",
    "        self.learn.loss_scale = 1./self.n_acc\n",
    "    \n",
    "    def after_backward(self):\n",
    "        if not self.training: return\n",
    "        # Nur nach n_acc Schritten optimieren\n",
    "        if (self.iter+1) % self.n_acc != 0:\n",
    "            # GPU Cache leeren\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            raise CancelBatchException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffcc9c0-7fa6-4374-8d3a-1ef8186e03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model_name, dataloader, disease_labels, bbox_csv, image_dir, output_manager):\n",
    "        self.model_name = model_name\n",
    "        self.dataloader = dataloader\n",
    "        self.disease_labels = disease_labels\n",
    "        self.bbox_csv = bbox_csv\n",
    "        self.image_dir = image_dir\n",
    "        self.output_manager = output_manager\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'valid_loss': [],\n",
    "            'learning_rates': [],\n",
    "            'scheduler_info': {}\n",
    "        }\n",
    "\n",
    "    def _clear_gpu_memory(self):\n",
    "        \"\"\"Bereinigt den GPU-Speicher\"\"\"\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    def get_model(self):\n",
    "        base_arch = timm.create_model(\n",
    "            'convnext_tiny.fb_in22k', \n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool=''\n",
    "        )\n",
    "        \n",
    "        n_features = base_arch.num_features\n",
    "        head = CustomHead(n_features, len(self.disease_labels))\n",
    "        model = MultiLabelModel(base_arch, head)\n",
    "        \n",
    "        # Erstelle den FastAI Learner und speichere ihn als Instanzvariable\n",
    "        self.learn = Learner(\n",
    "            self.dataloader,\n",
    "            model,\n",
    "            loss_func=nn.BCEWithLogitsLoss(),\n",
    "            metrics=[accuracy_multi]\n",
    "        )\n",
    "\n",
    "        # MixedPrecision (AMP) einschalten um GPU SPeichernutzung zu reduzieren\n",
    "        self.learn = self.learn.to_fp16()\n",
    "        \n",
    "        return self.learn\n",
    "\n",
    "    def find_learning_rate(self, model, suggestion_mode='steep'):\n",
    "        print(f\"Suche optimale Learning Rate für {self.model_name}...\")\n",
    "        try:\n",
    "            suggestions = model.lr_find(\n",
    "                start_lr=1e-7,\n",
    "                end_lr=1e-1,\n",
    "                num_it=50,\n",
    "                show_plot=True\n",
    "            )\n",
    "            # GPU-Speicher nach dem Learning Rate Finder leeren\n",
    "            self._clear_gpu_memory()\n",
    "            \n",
    "            return suggestions.valley\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim LR Finding: {str(e)}\")\n",
    "\n",
    "            # Auch bei Fehlerfall Speicher leeren\n",
    "            self._clear_gpu_memory()\n",
    "        \n",
    "            return 1e-4\n",
    "    def train(self, scheduler_config, epochs=None, freeze_epochs=2, resume_from_checkpoint=None, additional_epochs=0):\n",
    "        print(f\"Training {self.model_name}...\")\n",
    "        \n",
    "        # PyTorch Speichermanagement optimieren\n",
    "        import os\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "            \n",
    "        # Importiere traceback am Anfang der Funktion\n",
    "        import traceback\n",
    "\n",
    "        # GPU-Speicher nach dem Learning Rate Finder leeren\n",
    "        self._clear_gpu_memory()\n",
    "        \n",
    "        if epochs is None:\n",
    "            epochs = scheduler_config.get('epochs', 10)  # Nutze epochs aus config oder default 10\n",
    "            \n",
    "        # Wenn additional_epochs angegeben ist, erhöhe die Gesamtanzahl\n",
    "            if additional_epochs > 0:\n",
    "                print(f\"Erhöhe Anzahl der Epochen um {additional_epochs}\")\n",
    "                epochs += additional_epochs\n",
    "            \n",
    "            print(f\"Geplante Gesamtanzahl der Epochen: {epochs}\")\n",
    "        \n",
    "        if not hasattr(self, 'learn'):\n",
    "            self.learn = self.get_model()\n",
    "        \n",
    "        self.learn.add_cb(MultiLabelMetrics(\n",
    "            self.disease_labels,\n",
    "            pd.read_csv(self.bbox_csv),\n",
    "            self.image_dir,\n",
    "            self.output_manager\n",
    "        ))\n",
    "\n",
    "        # Learning rate für später festlegen\n",
    "        if scheduler_config.get('use_lr_finder', True):\n",
    "            lr = scheduler_config.get('base_lr', 1e-4)\n",
    "        else:\n",
    "            lr = scheduler_config.get('base_lr', 1e-4)\n",
    "        \n",
    "        # Sehr wichtig: Initialisiere den Optimizer BEVOR der Checkpoint geladen wird\n",
    "        if not hasattr(self.learn, 'opt') or self.learn.opt is None:\n",
    "            # Optimizer erstellen (ohne lr Parameter)\n",
    "            self.learn.create_opt()\n",
    "            print(\"Optimizer initialisiert\")\n",
    "        \n",
    "        # Wenn Checkpoint angegeben, lade diesen\n",
    "        start_epoch = 0\n",
    "        if resume_from_checkpoint:\n",
    "            try:\n",
    "                print(f\"Versuche Checkpoint zu laden: {resume_from_checkpoint}\")\n",
    "                \n",
    "                # Prüfe, ob die Datei ein state_dict oder ein vollständiger Checkpoint ist\n",
    "                checkpoint = torch.load(resume_from_checkpoint)\n",
    "                \n",
    "                # Prüfe das Format\n",
    "                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "                    # Vollständiger Checkpoint\n",
    "                    print(\"Vollständiger Checkpoint erkannt\")\n",
    "                    self.learn.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    \n",
    "                    # Lade Optimizer-Zustand\n",
    "                    if 'optimizer_state_dict' in checkpoint:\n",
    "                        self.learn.opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                    \n",
    "                    # Starte bei der nächsten Epoche\n",
    "                    if 'epoch' in checkpoint:\n",
    "                        start_epoch = checkpoint['epoch']\n",
    "                        print(f\"Training wird ab Epoche {start_epoch + 1} fortgesetzt\")\n",
    "                    \n",
    "                    # Lade Metriken, falls vorhanden\n",
    "                    if 'metrics' in checkpoint:\n",
    "                        self.learn.metrics_dict = checkpoint['metrics']\n",
    "                        print(\"Metriken wurden geladen\")\n",
    "                else:\n",
    "                    # Nur state_dict\n",
    "                    print(\"Nur Modell-Gewichte (state_dict) erkannt\")\n",
    "                    self.learn.model.load_state_dict(checkpoint)\n",
    "                \n",
    "                print(\"Checkpoint erfolgreich geladen!\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Laden des Checkpoints: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "    \n",
    "        try:\n",
    "            start_time = datetime.now()  # Start-Zeit messen\n",
    "            \n",
    "            # Learning rate finding, nur wenn kein Checkpoint geladen wurde\n",
    "            if start_epoch == 0 and scheduler_config.get('use_lr_finder', True):\n",
    "                suggested_lr = self.find_learning_rate(self.learn)\n",
    "                print(f\"Empfohlene Learning Rate: {suggested_lr:.2e}\")\n",
    "                lr = suggested_lr\n",
    "            else:\n",
    "                lr = scheduler_config.get('lr', 1e-4)\n",
    "            \n",
    "            print(\"\\nStarting training...\")\n",
    "            \n",
    "            # Training mit Gradient Accumulation\n",
    "            n_acc = scheduler_config.get('gradient_accumulation', 2)  # Default auf 2, auh mal 4 probieren\n",
    "            print(f\"Training mit Gradient Accumulation: {n_acc}\")\n",
    "            remaining_epochs = epochs - start_epoch\n",
    "            \n",
    "            print(f\"Start Epoche: {start_epoch}\")\n",
    "            print(f\"Verbleibende Epochen: {remaining_epochs}\")\n",
    "            \n",
    "            if remaining_epochs > 0:\n",
    "                self.learn.fit_one_cycle(\n",
    "                    remaining_epochs, \n",
    "                    slice(lr/25, lr),\n",
    "                    wd=scheduler_config.get('weight_decay', 0.01),\n",
    "                    cbs=[GradientAccumulation(n_acc=n_acc)]\n",
    "                )\n",
    "\n",
    "            # Modell speichern\n",
    "            model_filename = f\"{self.model_name}_final\"\n",
    "            \n",
    "            state_dict_path = self.output_manager.get_path('models', f\"{model_filename}_state_dict.pth\")\n",
    "            self.output_manager.ensure_dir_exists(state_dict_path)\n",
    "            torch.save(self.learn.model.state_dict(), state_dict_path)\n",
    "\n",
    "            training_time = (datetime.now() - start_time).total_seconds()  # Trainingszeit berechnen\n",
    "            \n",
    "            # History updaten\n",
    "            self.history.update({\n",
    "                'training_time': training_time,\n",
    "                'final_metrics': getattr(self.learn, 'metrics_dict', {})\n",
    "            })\n",
    "            \n",
    "            return self.learn\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler während des Trainings: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        finally:\n",
    "            self._clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7e8d7-6a9f-467e-b6cc-7e2c4dafea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training_config():\n",
    "    \"\"\"Erstellt die grundlegende Trainingskonfiguration\"\"\"\n",
    "    config = {\n",
    "        'paths': {\n",
    "            'IMAGE_DIR': \"../Dataset/images\",\n",
    "            'LABELS_CSV': \"../Dataset/Data_Entry_2017_v2020.csv\",\n",
    "            'BBOX_CSV': \"../Dataset/BBox_List_2017.csv\",\n",
    "            'AUGMENTED_DIR': \"../Dataset/augmented_images\"  # Neu: Verzeichnis für augmentierte Bilder\n",
    "        },\n",
    "        'model_config': {\n",
    "            'convnext_base': {\n",
    "                'batch_size': 12,\n",
    "                'image_size': 192, #statt 224\n",
    "                'base_lr': 7.59e-5,#1e-4\n",
    "                'num_workers': 6,\n",
    "                'prefetch_factor': 4\n",
    "            }\n",
    "        },\n",
    "        'scheduler_config': [{\n",
    "            'type': 'one_cycle',\n",
    "            'use_lr_finder': False,\n",
    "            'epochs': 10,\n",
    "            'pct_start': 0.3,\n",
    "            'div': 25,\n",
    "            'num_it': 50,              # Reduzierte Anzahl von Iterationen für LR-Finder\n",
    "            'gradient_accumulation': 2 # Neue Option, auh mal 4 probieren\n",
    "        }]\n",
    "    }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6869471-e01b-40ef-b76c-7c77f0b356a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training_environment(config, max_samples=None):\n",
    "    \"\"\"Initialisiert die Trainingsumgebung mit vorberechneten augmentierten Daten\"\"\"\n",
    "    # Überprüfen der Existenz der Dateien und Verzeichnisse\n",
    "    for path in config['paths'].values():\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Pfad nicht gefunden: {path}\")\n",
    "    print(\"Alle Pfade sind verfügbar.\")\n",
    "\n",
    "    # Output Manager erstellen\n",
    "    output_manager = OutputManager()\n",
    "    \n",
    "    # Konfiguration speichern\n",
    "    with open(output_manager.get_path('metrics', 'configuration.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    # Prüfe, ob augmentierte Daten existieren\n",
    "    aug_dir = config['paths'].get('AUGMENTED_DIR', '../Dataset/augmented_images')\n",
    "    aug_csv = os.path.join(aug_dir, 'augmented_labels.csv')\n",
    "    \n",
    "    # Dataset mit Originaldaten und augmentierten Daten erstellen\n",
    "    dataset = LungXRayDataset(\n",
    "        image_dir=config['paths']['IMAGE_DIR'],\n",
    "        labels_csv=config['paths']['LABELS_CSV'],\n",
    "        bbox_csv=config['paths']['BBOX_CSV'],\n",
    "        aug_dir=aug_dir if os.path.exists(aug_dir) else None,\n",
    "        aug_csv=aug_csv if os.path.exists(aug_csv) else None,\n",
    "        max_samples=max_samples\n",
    "    )\n",
    "    \n",
    "    return output_manager, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c906a-7590-49e3-9f38-1bf21bb02d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model_name, config, dataset, output_manager):\n",
    "    \"\"\"Erstellt und konfiguriert den ModelTrainer\"\"\"\n",
    "    model_config = config['model_config'][model_name]\n",
    "    \n",
    "    dataloader = dataset.create_dataloader(\n",
    "        batch_size=model_config['batch_size'],\n",
    "        image_size=model_config['image_size'],\n",
    "        num_workers=model_config['num_workers']\n",
    "        #prefetch_factor=model_config['prefetch_factor']\n",
    "    )\n",
    "    \n",
    "    trainer = ModelTrainer(\n",
    "        model_name, \n",
    "        dataloader, \n",
    "        dataset.disease_labels,\n",
    "        config['paths']['BBOX_CSV'],\n",
    "        config['paths']['IMAGE_DIR'],\n",
    "        output_manager\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc4127-401c-406b-8f1e-5e511b69b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hauptausfuehrungscode\n",
    "def main(resume_from=None, additional_epochs=0):\n",
    "    \"\"\"Hauptfunktion für Training und Fortsetzung\"\"\"\n",
    "    # GPU-Speicher bereinigen\n",
    "    clear_gpu_memory()\n",
    "    print(\"GPU-Speicher bereinigt\")\n",
    "    \n",
    "    # Konfiguration laden\n",
    "    config = setup_training_config()\n",
    "    \n",
    "    # Trainingsumgebung einrichten\n",
    "    output_manager, dataset = setup_training_environment(config)\n",
    "    \n",
    "    # Trainer für jedes Modell erstellen und trainieren\n",
    "    results = {}\n",
    "    for model_name in config['model_config'].keys():\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        model_results = {}\n",
    "        \n",
    "        trainer = create_trainer(model_name, config, dataset, output_manager)\n",
    "        \n",
    "        for scheduler_config in config['scheduler_config']:\n",
    "            print(f\"\\nVerwende {scheduler_config['type']} Scheduler...\")\n",
    "            \n",
    "            # Checkpoint-Pfad vorbereiten, wenn angegeben\n",
    "            checkpoint_path = None\n",
    "            if resume_from:\n",
    "                # Prüfe, ob resume_from ein Verzeichnis oder eine Datei ist\n",
    "                if os.path.isdir(resume_from):\n",
    "                    # Wenn ein Verzeichnis, suche den neuesten Checkpoint\n",
    "                    checkpoint_files = glob.glob(os.path.join(resume_from, \"*.pth\"))\n",
    "                    if checkpoint_files:\n",
    "                        checkpoint_path = max(checkpoint_files, key=os.path.getctime)\n",
    "                        print(f\"Neuester Checkpoint gefunden: {checkpoint_path}\")\n",
    "                    else:\n",
    "                        print(f\"Keine Checkpoint-Dateien in {resume_from} gefunden.\")\n",
    "                else:\n",
    "                    # Wenn keine Verzeichnis, nutze den Pfad direkt\n",
    "                    checkpoint_path = resume_from\n",
    "                    if not os.path.exists(checkpoint_path):\n",
    "                        print(f\"Warnung: Checkpoint {checkpoint_path} existiert nicht!\")\n",
    "            \n",
    "            # Training starten oder fortsetzen\n",
    "            model = trainer.train(\n",
    "                scheduler_config,\n",
    "                resume_from_checkpoint=checkpoint_path,\n",
    "                additional_epochs=additional_epochs\n",
    "            )\n",
    "            model_results[scheduler_config['type']] = trainer.history\n",
    "        \n",
    "        results[model_name] = model_results\n",
    "    \n",
    "    # Finale Ergebnisse speichern\n",
    "    with open(output_manager.get_path('metrics', 'final_results.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    # Plot der finalen Trainingsmetriken\n",
    "    if results and 'convnext_base' in results and 'one_cycle' in results['convnext_base']:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Loss Plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(results['convnext_base']['one_cycle']['train_loss'], label='Training Loss')\n",
    "        plt.plot(results['convnext_base']['one_cycle']['valid_loss'], label='Validation Loss')\n",
    "        plt.title('Training und Validation Loss')\n",
    "        plt.xlabel('Batch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Learning Rate Plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(results['convnext_base']['one_cycle']['learning_rates'])\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.xlabel('Batch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(output_manager.get_path('plots', 'final_training_metrics.png'))\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"Keine Trainingsergebnisse verfügbar für das Plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93537c0-8297-45ba-9dbb-df444ab3e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu_memory()\n",
    "print(\"Go! Go! Go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa43d5-a720-4639-bd2e-b8f91240141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\n",
    "    resume_from=\"run_20250227_180651/models\",#/checkpoint_epoch_5_20250220_180123.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295fb32-0920-4f49-8191-7e12d7c9c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de2ea2-a685-4e9c-801e-c566d7361bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Neues Training starten\n",
    "    main()\n",
    "    \n",
    "    # Oder: Training von Checkpoint fortsetzen\n",
    "    \n",
    "    # main(\n",
    "        #resume_from=\"run_20250213_135241/models\",\n",
    "        #additional_epochs=5\n",
    "    #)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19247665-28f9-4695-aca9-ccef9498a4a2",
   "metadata": {},
   "source": [
    "### Zum Laden eines Modells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb3492-6c6f-4f5b-8a77-f188c39d4549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nur Gewichte laden\n",
    "state_dict = torch.load(weights_path)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Vollständiges Modell laden\n",
    "checkpoint = torch.load(full_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
